# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18JkdnWafN4M4JYk3idQ8R168QtlSo5d4
"""

import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], enable = True)
from tensorflow import keras
from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer, SimpleRNN, LSTM, Flatten
from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential, Model
import numpy as np
import warnings
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)

print(physical_devices)

import os
import re
import numpy as np
import pandas as pd

# text treatement
import nltk
from nltk.tokenize import word_tokenize
import string
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re

nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

all_stopwords = stopwords.words('english')
all_stopwords.remove('not')


import csv,sys
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import GridSearchCV 
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn import svm 
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier 
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.feature_selection import SelectKBest,chi2

dataset_train = pd.read_csv('project2_training_data.txt', delimiter= '\r\n' , header = None, names=['text'])
dataset_labels = pd.read_csv('project2_training_data_labels.txt', delimiter= '\r\n' , header = None, names=['text'])

dataset_train.head()

dataset_train = dataset_train.dropna(axis='columns')

dataset_train.head()

dataset_labels.head()

result = pd.concat([dataset_train, dataset_labels], axis=1, join='inner')

result.head()

result.columns = ['Texts', 'Sentiment']

result.head()

df = result.copy()

df.head()

# Importing LabelEncoder from Sklearn
# library from preprocessing Module.
from sklearn.preprocessing import LabelEncoder
 
# Creating a instance of label Encoder.
le = LabelEncoder()
 
# Using .fit_transform function to fit label
# encoder and return encoded label
label = le.fit_transform(df['Sentiment'])
 
# printing label
label

# removing the column 'Purchased' from df
# as it is of no use now.
# df.drop("Sentiment", axis=1, inplace=True)
 
# Appending the array to our dataFrame
# with column name 'Purchased'
df["Class"] = label

# removing special characters
df = df.replace('\*', ' ')
df = df.replace('\xc2', ' ', regex=True)
df = df.replace('\xf1', ' ', regex=True)
df = df.replace('\xc1', ' ', regex=True)
df = df.replace('[^a-zA-Z]', ' ')
# df = df.replace('"+"', ' ', regex=True)
# df = df.replace('"+"', ' ', regex=True)

# printing Dataframe
df

max_len = int(df['Texts'].str.len().max())

max_len

df["Texts"]= df["Texts"].str.pad(301, side ='both')

df['Class'].unique()

#1 for neutral
#2 for positive
#0 for negative

df = df.dropna()

df.rename(columns = {'Texts':'text', 'Class':'target'}, inplace = True)

df.head()

from keras.utils import to_categorical

one_hot_encode = to_categorical(df.target)
print(one_hot_encode)
y = one_hot_encode

class TransformerBlock(Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential(
            [Dense(ff_dim, activation="relu"), 
             Dense(embed_dim),]
        )
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.ff_dim = ff_dim
        self.rate = rate

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

    def get_config(self):
      config = super().get_config().copy()
      config.update({'embed_dim': self.embed_dim, 'num_heads': self.num_heads, 'ff_dim': self.ff_dim, 'rate': self.rate})
      return config

class TokenAndPositionEmbedding(Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)
        
    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

vocab_size = 1900  # Only consider the top approx 2k words
maxlen = 200  # Only consider the first 200 words of each movie review

X = df.text

# Split original dataframe into train and temp dataframes.
df_train, df_test, y_train, y_test = train_test_split(X,
                                                      y,
                                                      stratify=y,
                                                      test_size=(0.3))

print(len(df_train), "Training sequences")
print(len(df_test), "Test sequences")

vocab_size = 1900  # Only consider the top approx 2k words
maxlen = 200  # Only consider the first 200 words of each movie review

X = df.text

# Split original dataframe into train and temp dataframes.
df_train, df_test, y_train, y_test = train_test_split(X,
                                                      y,
                                                      stratify=y,
                                                      test_size=(0.3))

print(len(df_train), "Training sequences")
print(len(df_test), "Test sequences")

from keras.preprocessing.text import Tokenizer
from collections import Counter

def counter_func(string_input):
  var1 = Counter()
   
  for line in string_input.values:
    for word in line.split():
      var1[word] += 1
  return var1

var2 = counter_func(df.text)

num_words = len(var2)
max_len_vec = 0
for line in df.text.values:
  if (len(line.split()) > (max_len_vec)):
    max_len_vec = len(line.split())
  
print(max_len_vec)

var3 = Tokenizer(num_words = num_words)
var3.fit_on_texts(df_train)
word_ind = var3.word_index

df_train = var3.texts_to_sequences(df_train)
df_test = var3.texts_to_sequences(df_test)

print(df_train)

df_train = tf.keras.preprocessing.sequence.pad_sequences(df_train, maxlen=max_len_vec)
df_test = tf.keras.preprocessing.sequence.pad_sequences(df_test, maxlen=max_len_vec)

df_train.shape, y_train.shape

df_test.shape, y_test.shape

y_train

y_train = y_train.astype('int32')
y_test = y_test.astype('int32')

"""## Choice thingy"""

choice = int(input("Pick your model: 1-RNN, 2-LSTM, 3-FFNN, 4-Transformer"))

if choice == 1:

  # fixing every word's embedding size to be 32
  embd_len = 32
  vocab_size =500

  # Creating a RNN model
  model = Sequential(name="Simple_RNN")
  model.add(Embedding(vocab_size,
                          embd_len,
                          input_length=df_train.shape[1]))

  # In case of a stacked(more than one layer of RNN)
  # use return_sequences=True
  model.add(SimpleRNN(128,
                          activation='tanh',
                          return_sequences=True))
  model.add(SimpleRNN(128,
                          activation='tanh',
                          return_sequences=True))

  model.add(SimpleRNN(128,
                          activation='tanh',
                          return_sequences=True))

  model.add(SimpleRNN(128,
                          activation='tanh',
                          return_sequences=False))

  model.add(Dense(3, activation='sigmoid'))

  # printing model summary
  print(model.summary())

  # Compiling model
  model.compile(
      loss="binary_crossentropy",
      optimizer='adam',
      metrics=['accuracy']
  )
  # Printing model score on test data
  # print()
  # print("Simple_RNN Score---> ", model.evaluate(x_test, y_test, verbose=0))

elif choice == 2:


    

  # fixing every word's embedding size to be 32
  embd_len = 32
  vocab_size =500

  # Creating a RNN model
  model = Sequential(name="LSTM")
  model.add(Embedding(vocab_size,
                          embd_len,
                          input_length=df_train.shape[1]))

  # In case of a stacked(more than one layer of RNN)
  # use return_sequences=True
  model.add(LSTM(128, activation='tanh', return_sequences=True))
  model.add(LSTM(128, activation='tanh', return_sequences=True))

  model.add(LSTM(128, activation='tanh', return_sequences=True))

  model.add(LSTM(128, activation='tanh', return_sequences=False))

  model.add(Dense(3, activation='sigmoid'))

  # printing model summary
  print(model.summary())

  # Compiling model
  model.compile(
      loss="binary_crossentropy",
      optimizer='adam',
      metrics=['accuracy']
  )
  # Printing model score on test data
  # print()
  # print("Simple_RNN Score---> ", model.evaluate(x_test, y_test, verbose=0))


elif choice == 3:

  # fixing every word's embedding size to be 32
  embd_len = 32
  vocab_size =500

  # Creating a RNN model
  model = Sequential(name="FFNN")
  model.add(Embedding(vocab_size,
                          embd_len,
                          input_length=df_train.shape[1]))

  # In case of a stacked(more than one layer of RNN)
  # use return_sequences=True
  model.add(Dense(128,
                          activation='tanh'))
  model.add(Dense(128,
                          activation='tanh'))

  model.add(Dense(128,
                          activation='tanh'))

  model.add(Dense(128,
                          activation='tanh'))
  model.add(Flatten())

  model.add(Dense(3, activation='sigmoid'))

  # printing model summary
  print(model.summary())

  # Compiling model
  model.compile(
      loss="binary_crossentropy",
      optimizer='adam',
      metrics=['accuracy']
  )
  # Printing model score on test data
  # print()
  # print("Simple_RNN Score---> ", model.evaluate(x_test, y_test, verbose=0))


elif choice == 4:


  embed_dim = 32  # Embedding size for each token
  num_heads = 2  # Number of attention heads
  ff_dim = 32  # Hidden layer size in feed forward network inside transformer

  inputs = Input(shape=(max_len_vec,))
  embedding_layer = TokenAndPositionEmbedding(max_len_vec, vocab_size, embed_dim)
  x = embedding_layer(inputs)
  transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
  x = transformer_block(x)
  x = transformer_block(x)
  x = transformer_block(x)
  x = transformer_block(x)
  x = transformer_block(x)
  x = GlobalAveragePooling1D()(x)
  x = Dropout(0.1)(x)
  x = Dense(20, activation="relu")(x)
  x = Dropout(0.1)(x)
  outputs = Dense(3, activation="softmax")(x)

  model = Model(inputs=inputs, outputs=outputs)

else:
  print('wrong input')

model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

model.summary()

history = model.fit(df_train, y_train, epochs=10, 
                    validation_data=(df_test, y_test)
                   )

"""# Prediction"""

dataset_test = pd.read_csv('project2_test_data.txt', delimiter= '\r\n' , header = None, names=['text'])

dataset_test.head()

df2 = dataset_test.copy()

# removing special characters
df2 = df2.replace('\*', ' ')
df2 = df2.replace('\xc2', ' ', regex=True)
df2 = df2.replace('\xf1', ' ', regex=True)
df2 = df2.replace('\xc1', ' ', regex=True)
df2 = df2.replace('[^a-zA-Z]', ' ')
# df = df.replace('"+"', ' ', regex=True)
# df = df.replace('"+"', ' ', regex=True)

df2

df2['text'] = var3.texts_to_sequences(df2['text'])

df2

df2['text'] = tf.keras.preprocessing.sequence.pad_sequences(df2['text'], maxlen=max_len_vec)

df2

predicts = model.predict(df2['text'])

print(predicts)

lst_pred = []

for element in predicts:
  element = np.argmax(element)
  lst_pred.append(element)

final_pred = le.inverse_transform(lst_pred)

"""# Predicted class labels

"""

final_pred

with open('class_labels.txt', 'w+') as f:
  #final_pred = str(final_pred)
  for pred in final_pred:
    f.write(pred + '\n')
